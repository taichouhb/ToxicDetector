{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.01388889 0.         0.\n",
      "  0.05555556 0.05555556 0.         0.         0.01388889 0.\n",
      "  0.         0.         0.01388889 0.         0.         0.\n",
      "  0.02777778 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.53846154 0.         0.         0.         0.07692308 0.\n",
      "  0.15384615 0.         0.07692308 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.25       0.3125     0.         0.         0.         0.\n",
      "  0.25       0.3125     0.         0.3125     0.3125     0.3125\n",
      "  0.3125     0.3125     0.3125    ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.10526316 0.         0.         0.         0.02631579 0.\n",
      "  0.         0.         0.15789474 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.125      0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16129032 0.06451613 0.         0.         0.03225806 0.\n",
      "  0.         0.06451613 0.03225806 0.06451613 0.06451613 0.06451613\n",
      "  0.06451613 0.06451613 0.06451613]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3853211  0.04587156 0.         0.         0.00917431 0.\n",
      "  0.09174312 0.04587156 0.04587156 0.04587156 0.04587156 0.04587156\n",
      "  0.04587156 0.04587156 0.04587156]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.17073171 0.         0.         0.         0.12195122 0.\n",
      "  0.04878049 0.         0.07317073 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#turn train set and test set inputs to feature vectors \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "root_dir = os.path.abspath('.')\n",
    "data_dir = os.path.join(root_dir, 'dataset')\n",
    "train = pd.read_csv(os.path.join(data_dir,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "test_x, _ = utils.featurize(test, stage='test')\n",
    "\n",
    "\n",
    "train_x, train_y = utils.featurize(train)\n",
    "print(test_x[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for tree and random forest classiifers that return the probability of labels \n",
    "def get_proba(clf,x):\n",
    "    a = []\n",
    "    for v in clf.predict_proba(x.reshape(1,-1)):\n",
    "        a.append([v[0,1]])\n",
    "       \n",
    "    return a\n",
    "#return the ith column of the input matrix\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905979162568257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5-fold cross validation for tree classifier\n",
    "#since it is multi-label, tree.score is using harsh metrics\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=3)\n",
    "cv_results = cross_validate(tree,train_x,train_y,cv=5,return_estimator=True,return_train_score=False)\n",
    "np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90202099 0.90524535 0.90646738 0.90477533 0.90606004]\n"
     ]
    }
   ],
   "source": [
    "#use the first classifier of the 5 fold\n",
    "#output the predicited probaility to \"decision_tree.csv\"\n",
    "clf=cv_results[\"estimator\"][0]\n",
    "\n",
    "f = open(\"decision_tree.csv\",\"w\")\n",
    "k = 0\n",
    "for feat in test_x:\n",
    "    prob = get_proba(clf,feat)\n",
    "    f.write(str(test.values[k][0]) + \",\" +\",\".join(str(x[0]) for x in prob))\n",
    "    f.write(\"\\n\")\n",
    "    k+=1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90747298 0.90891145 0.91032149 0.91104218 0.91050949]\n"
     ]
    }
   ],
   "source": [
    "#5-fold cross validation for tree classifier\n",
    "#since it is multi-label, tree.score is using harsh metrics\n",
    "rf = RandomForestClassifier(n_estimators=300,min_samples_leaf=4)\n",
    "rf_cv_results=cross_validate(rf,train_x,train_y,cv=5,return_estimator=True,return_train_score=False)\n",
    "print(rf_cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 1 1 0 1 0]\n",
      "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
      "[0.    0.    0.    0.    0.    0.    4.625 0.25  0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Hey... what is it..\n",
      "@ | talk .\n",
      "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n",
      "\n",
      "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n",
      "[0.   0.   0.   0.   0.   0.   0.86 0.   0.   0.   0.02 0.   0.08 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Bye! \n",
      "\n",
      "Don't look, come or think of comming back! Tosser.\n",
      "[0.  0.  0.  0.  0.  0.  0.3 0.  0.2 0.  0.1 0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.  0.  0. ]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "You are gay or antisemmitian? \n",
      "\n",
      "Archangel WHite Tiger\n",
      "\n",
      "Meow! Greetingshhh!\n",
      "\n",
      "Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n",
      "\n",
      "1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n",
      "\n",
      "2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n",
      "\n",
      "3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n",
      "\n",
      "Beware of the Dark Side!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.1981982  0.00900901 0.06306306 0.         0.06306306 0.\n",
      " 0.12612613 0.         0.05405405 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "I'm Sorry \n",
      "\n",
      "I'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But then again, I'm going to go play outside....with your mom.   76.122.79.82\n",
      "[0.         0.         0.         0.01612903 0.         0.\n",
      " 0.16129032 0.09677419 0.         0.         0.01612903 0.\n",
      " 0.03225806 0.11290323 0.03225806 0.09677419 0.09677419 0.09677419\n",
      " 0.09677419 0.09677419 0.09677419]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 0 1 0 0 0]\n",
      "GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP.  I'M FUCKED UP RIGHT NOW!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 3.2962963  0.22222222 0.03703704 0.         0.         0.\n",
      " 0.         0.07407407 0.         0.07407407 0.07407407 0.07407407\n",
      " 0.07407407 0.07407407 0.07407407]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 1 1 0 1 0]\n",
      "Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!\n",
      "[0.         0.         0.15789474 0.         0.         0.\n",
      " 0.05263158 0.26315789 0.05263158 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Why can't you believe how fat Artie is? Did you see him on his recent appearence on the Tonight Show with Jay Leno? He looks absolutely AWFUL! If I had to put money on it, I'd say that Artie Lange is a can't miss candidate for the 2007 Dead pool!   \n",
      "\n",
      "  \n",
      "Kindly keep your malicious fingers off of my above comment, . Everytime you remove it, I will repost it!!!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.30136986 0.05479452 0.06849315 0.         0.04109589 0.\n",
      " 0.04109589 0.05479452 0.02739726 0.05479452 0.05479452 0.05479452\n",
      " 0.05479452 0.05479452 0.05479452]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 1 0 1 0]\n",
      "All of my edits are good.  Cunts like you who revert good edits because you're too stupid to understand how to write well , and then revert other edits just because you've decided to bear a playground grudge, are the problem.  Maybe one day you'll realise the damage you did to a noble project.  201.215.187.159\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.05172414 0.06896552 0.         0.         0.03448276 0.\n",
      " 0.         0.06896552 0.05172414 0.06896552 0.06896552 0.06896552\n",
      " 0.06896552 0.06896552 0.06896552]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "That's what I'm looking through, it looks like he was just being an all around dick on talk pages though. It's nothing too bad though, looks more like an inability to express himself properly. Thank you for your concern also, Cheers!! -\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.11904762 0.02380952 0.04761905 0.         0.07142857 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 1 0 0]\n",
      "Hi! I am back again!\n",
      "Last warning!\n",
      "Stop undoing my edits or die!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.36363636 0.         0.36363636 0.         0.         0.\n",
      " 0.18181818 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 1 0]\n",
      "Would you both shut up, you don't run wikipedia, especially a stupid kid.\n",
      "[0.         0.         0.07692308 0.         0.         0.\n",
      " 0.07692308 0.         0.         0.         0.15384615 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "A pair of jew-hating weiner nazi schmucks.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.14285714 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.14285714 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "From what I've seen with editors other than Eric, the elephant in the room is the issue of baiting/poking the bear/whatever the hell you want to call passive/aggressive baiting. No one wants to touch that one, even though it's uncivil as hell and appears to be the tactic of choice whenever someone wants to get rid of someone or get their way. That's the part that saddens me. darkness\n",
      "[0.         0.         0.02898551 0.         0.         0.\n",
      " 0.07246377 0.02898551 0.         0.         0.02898551 0.\n",
      " 0.         0.         0.13043478 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "\"\n",
      "\n",
      "SORRY PUCK BUT NO ONE EVER SAID DICK WAS NUMBER ONE!! But he is clearly listed as being \"\"second only to Batman\"\" and a \"\"master detective and martial artist\"\" and been trained in all the mentioned combat discplines. No need to mention in skills/abilities about the battles he's lost and won because thats not what an encylopia is supposed to do. \n",
      "\n",
      "Also, what your problem is your going by low end showings and not HIGH end showings where he's proven more than a match for a meta like Slade and pinned Shiva down in one of his earliest appearences.\n",
      " \"\n",
      "[0.   0.   0.   0.   0.   0.   0.51 0.03 0.02 0.   0.01 0.   0.05 0.03\n",
      " 0.04 0.02 0.02 0.02 0.02 0.02 0.02]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "\"\n",
      "\n",
      "UNBLOCK ME OR I'LL GET MY LAWYERS ON TO YOU FOR BLOCKING MY CONSTITUTIONAL RIGHT TO FREE SPEECH\"\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 4.27777778 0.         0.         0.         0.         0.\n",
      " 0.11111111 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 1 0]\n",
      "You should be fired, you're a moronic wimp who is too lazy to do research. It makes me sick that people like you exist in this world.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.07407407 0.         0.         0.         0.03703704 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [1 0 1 1 1 1]\n",
      "I think that your a Fagget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms\n",
      "[0.         0.         0.03448276 0.         0.         0.\n",
      " 0.13793103 0.03448276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 1 0]\n",
      "REPLY ABOVE:\n",
      "That was me, loser. The UN defines Vietnam to be part of Southeast Asia. And far as I know Vietnam is part of ASEAN, and used to be part of French Indochina with Laos and all those shit countries Anyway your culture has always been more influenced by SEA than Han Chinese (as in proper Yangtze Han Chinese, not the fringe indigenous tribes in Guangzhou/Guangxi). \n",
      "\n",
      "Just admit that you vietnamese are all a bunch of wannabe crap people. ALL the east asian people I've spoken to thinks of Vietnam as a very integral part of SEA, and we all think you're backward, dirty and speak in a horrible swearing language. Doesn't matter what crap you spout on Wikipedia, won't change the way people in the real world think.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.3875969  0.00775194 0.         0.         0.04651163 0.\n",
      " 0.02325581 0.         0.03875969 0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 1 1 0 1 0]\n",
      "you are a stupid fuck \n",
      "\n",
      "and your mother's cunt stinks\n",
      "[0.  0.1 0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.1 0.  0. ]\n",
      "predicted: [[1 0 1 0 0 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "Kill all niggers. \n",
      "\n",
      "I have hard, that others have said this.. should this be included? That racists sometimes say these.\n",
      "[0.   0.   0.   0.   0.   0.   0.15 0.05 0.   0.   0.05 0.   0.1  0.\n",
      " 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "Well I will use my user page then, don't tell meyour gonna remove it too, because that will piss me off and drive me nuts. Also I am not making a personal ttack on Jimmy wales, I am saying he is a bit to greedy dont you think? He's got 6 million dollars already and now he's asking for more.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.11666667 0.01666667 0.         0.         0.05       0.\n",
      " 0.         0.01666667 0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "u r a tw@ fuck off u gay boy.U r smelly.Fuck ur mum poopie\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.14285714 0.07142857 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Atheism is full of bias shit\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.16666667 0.16666667 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Hey why you are spreading misconceptions and trying to spread false information to the people.You not even know who won the battle and who lost.you are the cheapest person on the earth.Bloody ignorant fool.Check sources before spreading rumors\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.10526316 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.13157895 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 0 1 0 0 0]\n",
      "Fuck off\n",
      "\n",
      "You are NOT an administrator. You DON'T have the authority to tell me what to do.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.58823529 0.05882353 0.         0.         0.         0.\n",
      " 0.11764706 0.         0.05882353 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Well you are ridiculous, in fact I suspect that you are Calton, please block me, I dont care....\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.22222222 0.         0.         0.         0.16666667 0.\n",
      " 0.         0.         0.05555556 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 1 0]\n",
      "Keep your eye on it, no one cares, OK? People can put what they want on here, and clean it up all you want but no one cares. God, you're so freakin' stupid!   Omnigan\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.16666667 0.05555556 0.02777778 0.         0.11111111 0.\n",
      " 0.         0.05555556 0.02777778 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 1 0 0 0]\n",
      "MATT HARDY IS SO FUCKY!!!''Italic text[[Media:Example.ogg]]'''''MATT HARDY IS SO FUCKY!!!\n",
      "[0.  0.  0.  0.  0.  0.  3.9 0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0. ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "God is dead\n",
      "I don't mean to startle anyone but God is dead. We should not worry about him anymore. Just thought I would let everyone know. Well, goodbye and good luck with your newfound crisis of faith! 24.77.205.229\n",
      "[0.         0.         0.02631579 0.         0.         0.\n",
      " 0.18421053 0.         0.02631579 0.         0.02631579 0.\n",
      " 0.02631579 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Have you seen the reference to their claim, you moron''?? This is not a source. It´s their POV. Once I have more time I will reduce your Pashtun nationalist POVs here on Wiki and give order to my uncles and cousins the execution of Pashtuns in the entire northern Afghanistan down to Gardez, Jalalabad, Lashkargah, Logar, Paghman, Laghman, Kunduz, Herat, Farah ... whereever immigrant descandants of violent Kuchis settled and took non-Pashtun lands away and traditional Tajik countries. 84.59.190.210\n",
      "[0.         0.         0.01265823 0.         0.         0.\n",
      " 0.35443038 0.01265823 0.         0.         0.11392405 0.\n",
      " 0.         0.02531646 0.03797468 0.01265823 0.01265823 0.01265823\n",
      " 0.01265823 0.01265823 0.01265823]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Thank you \n",
      "Hei, it's me, the William Hope fan. Guess what - I am back at a new IP, ready to vandalize everything regarding William Hope, and you can't do sh-t to stop me, maybe delay me, but no stopping me. So lets salute to some more William Hope lunacy by shoving his p-nis in my mouth.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.22807018 0.         0.         0.         0.10526316 0.\n",
      " 0.01754386 0.         0.01754386 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "That's a rather feeble premise to argue showing an excessive number of photos prominently showing 24 different women's vaginas, compared to one human male penis 3/4 of the way down the page on the equivalent article for male genitalia. This demonstrates the overwhelming male bias in editorial content on Wikipedia. 86.13.182.103\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.05882353 0.01960784 0.         0.         0.01960784 0.\n",
      " 0.         0.01960784 0.07843137 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "Ho ho ho! Merry Christmas \n",
      "\n",
      "I shoulda known the Piccirilli Brothers actiually executed Ward's pediment sculpture at the Stock Exchange! Happy Christmas, Carptrash.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.52173913 0.13043478 0.08695652 0.         0.04347826 0.\n",
      " 0.08695652 0.         0.08695652 0.         0.         0.\n",
      " 0.13043478 0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 1 0]\n",
      "Dear Mokele,\n",
      "You have no right to tell people to die in a fire. Just because that IP address is being used for vandilism does not mean you can talk to people like that. You are in fact a bigger loser than the person who you block.\n",
      "[0.         0.         0.02173913 0.         0.         0.\n",
      " 0.15217391 0.         0.         0.         0.02173913 0.\n",
      " 0.02173913 0.         0.04347826 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 0 0]]\n",
      "expected: [1 0 1 0 1 0]\n",
      "Thank you for your RACIST experimenting with the page Katherine Heigl on Wikipedia. Your test worked, and it has been reverted or removed. Please use the sandbox for any other tests you may want to do. Take a look at the welcome page to learn more about contributing to our encyclopedia. Now go FUCK YOURSELF YOU RACIST CANUCK ASSHOLE1  U HAVE NOTHING BETTER TO DO THAN CENSOR THE TRUTH U GODDAMN KKKKANUCK PIECE O TRASH///  DICKHEAD MORON MOTHERFUCKER111LOL  YOU CANUCKS R REAL RETARDS1  FUK OFF DICKWEED!  EDIT ANOTHER 1 OF MY EIDTS AND YOU'LL BE EXPEL FROM WIKI BY JIMBO YOU SOB DICKHEAD11 \n",
      "SO WATCH IT MOFO PIECE OF GARBAGE RACIST MOFO ASSHOLE.  IM TRYINBG TO BE NICE HERE SO TAKE WHAT i SAY AS A COMPLIMENT!Yamla\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 2.52631579 0.08270677 0.01503759 0.         0.0075188  0.\n",
      " 0.0075188  0.05263158 0.04511278 0.04511278 0.04511278 0.04511278\n",
      " 0.04511278 0.04511278 0.04511278]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "You must be logged in to upload files. \n",
      "\n",
      "So In other words I have to sign up just to add 3 things to 1 entry.\n",
      "\n",
      "To hell with that!!!!! This place can ROT for all I care!!!\n",
      "\n",
      "75.8.35.177\n",
      "[0.         0.         0.02777778 0.         0.         0.\n",
      " 0.27777778 0.02777778 0.22222222 0.         0.         0.\n",
      " 0.16666667 0.         0.02777778 0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "Salutations J.A.P., welcome to Wikipedia. Perhaps you might find WP:HOCKEY of interest.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 1.16666667 0.08333333 0.         0.         0.08333333 0.\n",
      " 0.         0.         0.08333333 0.08333333 0.         0.\n",
      " 0.         0.         0.08333333]\n",
      "predicted: [[1 0 1 0 0 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "Gay \n",
      "\n",
      "The existence of CDVF is further proof that  is a sad twat. He is also very ugly, and has a willy for a face.\n",
      "[0.         0.         0.03846154 0.         0.         0.\n",
      " 0.26923077 0.07692308 0.         0.         0.03846154 0.\n",
      " 0.07692308 0.03846154 0.         0.03846154 0.03846154 0.03846154\n",
      " 0.07692308 0.03846154 0.03846154]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 1 1 0 1 0]\n",
      "Hi \n",
      "\n",
      "Im a fucking bitch.\n",
      "\n",
      "50.180.208.181\n",
      "[0.  0.  0.  0.  0.  0.  0.4 0.2 0.  0.  0.  0.  0.8 0.  0.  0.  0.  0.\n",
      " 0.  0.  0. ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "Add Mobile porn as additional WP:ALSO link\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 1.14285714 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 1 0]\n",
      "\"\"\"Nazi filth\"\" is impolite  04:27, 20 Jan 2004 (UTC)\n",
      "\n",
      "\"\n",
      "[0.  0.  0.  0.  0.  0.  0.5 0.1 0.  0.  0.1 0.  0.2 0.1 0.1 0.1 0.1 0.1\n",
      " 0.1 0.1 0.1]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "I don't care what you say here. I don't believe one sentence anymore.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.15384615 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "I can tell you what the argument is about here newseventsguy. It's about YOUR retroactive reversion of a simple edit trying to include secondary independent verification of sedimentary nanodiamonds in Younger Dryas sediments by Bement and Madden et al., WITHOUT DISCUSSION a reference that I notice has NOT YET been included in the reference list still, after all this time. This is after you ADMIT that you are ignorant of the peer review literature and the controversy. Any attempt to do simple reference list edits on this page have been stomped by no less that five admittedly biased and ignorant editors. The honorable thing to do here would be to recuse yourself from editing the page, as I have. But I haven't seen that happening until now. Good luck with it, you've already made complete fools of yourselves.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.33333333 0.         0.         0.         0.02898551 0.\n",
      " 0.         0.         0.05797101 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 0 1 0 0 0]\n",
      "HELLO HOW DO I GET SOMEBODY TO FIX SHIT AROUND HERE?\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 3.72727273 0.09090909 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 1 0 1 1]\n",
      "Dictionaries\n",
      "\n",
      "How dare you call my contribution spam!!! I am a Kurd and I made a lsit of kurdish dictionaries. you bloody turkish nationalist and atoricity commiting bone breaking Nazi. watch out folk this slimy Turk is trying to censor the internet this is not undemocratic Turkey here, no prison cells in wikipedia you stupid Turk! And you buggers want membership to the EEC\n",
      "[0.         0.         0.01587302 0.         0.         0.\n",
      " 0.20634921 0.         0.06349206 0.         0.01587302 0.\n",
      " 0.03174603 0.         0.06349206 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [0 0 1 0 0 0]\n",
      "How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.14285714 0.0952381  0.         0.         0.0952381  0.\n",
      " 0.         0.14285714 0.         0.0952381  0.0952381  0.0952381\n",
      " 0.0952381  0.0952381  0.0952381 ]\n",
      "predicted: [[0 0 0 0 0 0]]\n",
      "expected: [1 0 0 0 0 0]\n",
      "You are a know it all.....Jimbo! \n",
      "\n",
      "Why don't you get a life instead of sitting in front of your computer all day. The high priestess of knowledge, indeed!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.14285714 0.         0.07142857 0.         0.03571429 0.\n",
      " 0.07142857 0.         0.03571429 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 0 0 0 0]]\n",
      "expected: [0 0 0 0 0 0]\n",
      "Vince - FYI 'Yellowfrogs' is (Longdendale) Councillor Sean Parker Perry, a frequent purveyor of 'Greenwash' and four square behind the destruction of Swallows Wood via his support for the bypass. He is vigourously re-editing the wikpedia pages on the Longdendale Bypass, his own and Roy Oldham's page in order to boost his fake 'green' credentials and obfuscate the neutrality of these articles. He gets nasty if he gets pissed off (threatens spurious legal action), but fortunately, he's legally illiterate, so don't be put off.\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.22619048 0.02380952 0.         0.         0.05952381 0.\n",
      " 0.         0.         0.07142857 0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "predicted: [[1 0 1 0 1 0]]\n",
      "expected: [1 1 1 0 1 0]\n",
      "What a motherfucking piece of crap those fuckheads for blocking us!\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.09090909 0.09090909 0.09090909 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#output 50 failed classified inputs\n",
    "idx = 0\n",
    "count = 0\n",
    "while(count<50):\n",
    "    a = rf_cv_results['estimator'][4].predict(train_x[idx].reshape(1,-1))\n",
    "    if(not (a.astype(int)==train_y[idx]).all()):\n",
    "        print(\"predicted: \",end=\"\")\n",
    "        print(a.astype(int))\n",
    "        print(\"expected: \",end=\"\")\n",
    "        print(train_y[idx])\n",
    "        print(train.values[idx][1])\n",
    "        print(train_x[idx])\n",
    "        count+=1\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search to find the best hyperparameter\n",
    "ran_f = RandomForestClassifier()\n",
    "parameters = {'n_estimators':[300,400,500,600,700],'min_samples_leaf': [4, 8, 10],'criterion': ['gini', 'entropy']}\n",
    "rfs = GridSearchCV(ran_f, parameters, cv=5)\n",
    "rfs.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([139.49301476, 183.4048697 , 216.53961377, 254.49498482,\n",
       "        292.28981099, 117.83096762, 157.21645169, 195.81741047,\n",
       "        245.992558  , 270.15040126, 113.42047105, 153.15201044,\n",
       "        188.70641985, 229.07230639, 270.33021464, 159.84373984,\n",
       "        213.80976849, 265.40656219, 316.66983218, 368.71965418,\n",
       "        149.28168707, 199.56126842, 248.52116828, 303.34603605,\n",
       "        372.69907465, 158.71623244, 211.16108875, 261.90978212,\n",
       "        293.78175302, 342.21890721]),\n",
       " 'mean_score_time': array([4.13468742, 5.52610269, 6.81144938, 8.4307889 , 9.59236431,\n",
       "        3.9394989 , 5.24314942, 6.56454639, 7.97624359, 9.19871073,\n",
       "        3.86701045, 5.25098157, 6.44697599, 7.77066789, 9.08452568,\n",
       "        3.98445449, 5.27895036, 6.600809  , 7.98061357, 9.26132555,\n",
       "        3.82501984, 5.12159362, 6.39070044, 7.69368873, 8.95484424,\n",
       "        3.84083858, 5.09334893, 6.4804215 , 7.60905185, 8.84001923]),\n",
       " 'mean_test_score': array([0.9096327 , 0.90964524, 0.9096891 , 0.90957003, 0.90962644,\n",
       "        0.90963897, 0.90969537, 0.9096139 , 0.90967657, 0.90963897,\n",
       "        0.9095199 , 0.90941963, 0.90954497, 0.90952617, 0.9095199 ,\n",
       "        0.90976431, 0.90962017, 0.90981444, 0.90977057, 0.90974551,\n",
       "        0.9095951 , 0.9094071 , 0.90951363, 0.90946977, 0.90956377,\n",
       "        0.90945097, 0.90932563, 0.90935696, 0.90947603, 0.90933816]),\n",
       " 'mean_train_score': array([0.91786258, 0.91788765, 0.91779991, 0.91783125, 0.91787512,\n",
       "        0.91338809, 0.91333638, 0.91336302, 0.91333012, 0.91329722,\n",
       "        0.9125076 , 0.9125452 , 0.9125781 , 0.91250133, 0.91252013,\n",
       "        0.91774351, 0.91777485, 0.91779991, 0.91776231, 0.91775448,\n",
       "        0.91325022, 0.91320478, 0.91318755, 0.91329565, 0.91329565,\n",
       "        0.9123431 , 0.912376  , 0.91241203, 0.91241673, 0.91235093]),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 4, 4,\n",
       "                    4, 4, 4, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[300, 400, 500, 600, 700, 300, 400, 500, 600, 700, 300,\n",
       "                    400, 500, 600, 700, 300, 400, 500, 600, 700, 300, 400,\n",
       "                    500, 600, 700, 300, 400, 500, 600, 700],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'criterion': 'gini', 'min_samples_leaf': 4, 'n_estimators': 300},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 4, 'n_estimators': 400},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 4, 'n_estimators': 500},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 4, 'n_estimators': 600},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 4, 'n_estimators': 700},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 8, 'n_estimators': 300},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 8, 'n_estimators': 400},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 8, 'n_estimators': 500},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 8, 'n_estimators': 600},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 8, 'n_estimators': 700},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 10, 'n_estimators': 300},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 10, 'n_estimators': 400},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 10, 'n_estimators': 500},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 10, 'n_estimators': 600},\n",
       "  {'criterion': 'gini', 'min_samples_leaf': 10, 'n_estimators': 700},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 4, 'n_estimators': 300},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 4, 'n_estimators': 400},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 4, 'n_estimators': 500},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 4, 'n_estimators': 600},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 4, 'n_estimators': 700},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 8, 'n_estimators': 300},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 8, 'n_estimators': 400},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 8, 'n_estimators': 500},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 8, 'n_estimators': 600},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 8, 'n_estimators': 700},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 10, 'n_estimators': 300},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 10, 'n_estimators': 400},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 10, 'n_estimators': 500},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 10, 'n_estimators': 600},\n",
       "  {'criterion': 'entropy', 'min_samples_leaf': 10, 'n_estimators': 700}],\n",
       " 'rank_test_score': array([11,  8,  6, 16, 12,  9,  5, 14,  7,  9, 20, 26, 18, 19, 20,  3, 13,\n",
       "         1,  2,  4, 15, 27, 22, 24, 17, 25, 30, 28, 23, 29], dtype=int32),\n",
       " 'split0_test_score': array([0.90706564, 0.90731631, 0.90703431, 0.90697164, 0.90728498,\n",
       "        0.90737898, 0.90731631, 0.90731631, 0.90750431, 0.90731631,\n",
       "        0.90728498, 0.90690898, 0.90725364, 0.90725364, 0.90728498,\n",
       "        0.90741031, 0.90719098, 0.90744164, 0.90725364, 0.90728498,\n",
       "        0.90712831, 0.90700298, 0.90722231, 0.90712831, 0.90731631,\n",
       "        0.90722231, 0.90694031, 0.90731631, 0.90712831, 0.90734764]),\n",
       " 'split0_train_score': array([0.91842138, 0.91829605, 0.91833521, 0.91835871, 0.91849188,\n",
       "        0.91394842, 0.91378392, 0.91382309, 0.91395626, 0.91388576,\n",
       "        0.91321207, 0.9132199 , 0.91318074, 0.91311023, 0.91309457,\n",
       "        0.91823338, 0.91818638, 0.91835088, 0.91817071, 0.91824121,\n",
       "        0.91380742, 0.91380742, 0.91369775, 0.91387009, 0.91387009,\n",
       "        0.91293006, 0.91299273, 0.9130319 , 0.91303973, 0.91294573]),\n",
       " 'split1_test_score': array([0.90935013, 0.90909945, 0.90919346, 0.90897412, 0.90897412,\n",
       "        0.90919346, 0.90875478, 0.90894278, 0.90897412, 0.90872344,\n",
       "        0.90884878, 0.90897412, 0.90875478, 0.90878611, 0.90875478,\n",
       "        0.90900545, 0.90903679, 0.90906812, 0.90909945, 0.90916212,\n",
       "        0.90900545, 0.90906812, 0.90875478, 0.90903679, 0.90938146,\n",
       "        0.90888012, 0.90900545, 0.90897412, 0.90922479, 0.90888012]),\n",
       " 'split1_train_score': array([0.91786584, 0.91797551, 0.91792068, 0.91788151, 0.91785801,\n",
       "        0.91358092, 0.91340075, 0.91358092, 0.91343209, 0.91340859,\n",
       "        0.91275057, 0.91272707, 0.91281324, 0.91271924, 0.91278191,\n",
       "        0.91794418, 0.91785801, 0.91782668, 0.91794418, 0.91770917,\n",
       "        0.91345559, 0.91342425, 0.91343209, 0.91354176, 0.91355742,\n",
       "        0.9124764 , 0.91257824, 0.91270357, 0.91275057, 0.91260174]),\n",
       " 'split2_test_score': array([0.91029015, 0.91047816, 0.91072883, 0.91044683, 0.91041549,\n",
       "        0.91019615, 0.91013348, 0.91016482, 0.91032149, 0.91025882,\n",
       "        0.91013348, 0.90985148, 0.91035282, 0.91022749, 0.91003948,\n",
       "        0.91072883, 0.91044683, 0.9107915 , 0.91057216, 0.91057216,\n",
       "        0.91072883, 0.91019615, 0.91041549, 0.91029015, 0.91035282,\n",
       "        0.91029015, 0.91050949, 0.91010215, 0.91016482, 0.90994548]),\n",
       " 'split2_train_score': array([0.91780318, 0.91777184, 0.9174585 , 0.91760734, 0.91759167,\n",
       "        0.91315008, 0.91304041, 0.91325192, 0.91324408, 0.91322058,\n",
       "        0.91224923, 0.9122649 , 0.91228056, 0.91216306, 0.91220223,\n",
       "        0.91762301, 0.91761517, 0.91766217, 0.9174585 , 0.91764651,\n",
       "        0.91300125, 0.91299341, 0.91292291, 0.91309525, 0.91293074,\n",
       "        0.91206906, 0.91210823, 0.91207689, 0.91195156, 0.91205339]),\n",
       " 'split3_test_score': array([0.91076017, 0.91076017, 0.9108855 , 0.91085417, 0.91085417,\n",
       "        0.91085417, 0.91148085, 0.91097951, 0.91101084, 0.91113618,\n",
       "        0.91097951, 0.91107351, 0.91082284, 0.91107351, 0.91113618,\n",
       "        0.91113618, 0.91094817, 0.91113618, 0.91094817, 0.91104218,\n",
       "        0.91097951, 0.91076017, 0.91082284, 0.9106035 , 0.9106975 ,\n",
       "        0.9106975 , 0.91013348, 0.91025882, 0.91054083, 0.91044683]),\n",
       " 'split3_train_score': array([0.91754467, 0.91767784, 0.91761517, 0.91769351, 0.91780318,\n",
       "        0.91318142, 0.91326758, 0.91311091, 0.91299341, 0.91299341,\n",
       "        0.9122179 , 0.91218656, 0.9122649 , 0.91210039, 0.91220223,\n",
       "        0.91726266, 0.91763084, 0.91766217, 0.91756034, 0.91758384,\n",
       "        0.91282108, 0.91289941, 0.91283674, 0.91285241, 0.91296991,\n",
       "        0.91196722, 0.91197506, 0.91196722, 0.91202989, 0.91188889]),\n",
       " 'split4_test_score': array([0.9106975 , 0.91057216, 0.9106035 , 0.9106035 , 0.9106035 ,\n",
       "        0.91057216, 0.9107915 , 0.91066617, 0.91057216, 0.91076017,\n",
       "        0.91035282, 0.91029015, 0.91054083, 0.91029015, 0.91038416,\n",
       "        0.91054083, 0.91047816, 0.91063483, 0.91097951, 0.91066617,\n",
       "        0.91013348, 0.91000815, 0.91035282, 0.91029015, 0.91007082,\n",
       "        0.91016482, 0.91003948, 0.91013348, 0.91032149, 0.91007082]),\n",
       " 'split4_train_score': array([0.91767784, 0.91771701, 0.91767001, 0.91761517, 0.91763084,\n",
       "        0.91307958, 0.91318925, 0.91304825, 0.91302475, 0.91297775,\n",
       "        0.91210823, 0.91232757, 0.91235107, 0.91241373, 0.91231973,\n",
       "        0.91765434, 0.91758384, 0.91749767, 0.91767784, 0.91759167,\n",
       "        0.91316575, 0.91289941, 0.91304825, 0.91311875, 0.91315008,\n",
       "        0.91227273, 0.91222573, 0.91228056, 0.9123119 , 0.9122649 ]),\n",
       " 'std_fit_time': array([ 2.9622114 ,  1.38154888, 11.93790799,  2.68365574,  0.8501712 ,\n",
       "         1.07975926,  0.6121706 ,  1.74162209, 16.54879914,  2.08519638,\n",
       "         0.92307433,  1.32745902,  0.80983355,  1.56368405,  1.74863869,\n",
       "         0.83284664,  1.50376288,  2.78606091,  2.13709103,  2.3722143 ,\n",
       "         1.14417298,  2.96571112,  1.14627407, 10.79753458, 19.90626935,\n",
       "         0.70036467,  1.19436908,  2.27512337,  4.31828134,  4.98183635]),\n",
       " 'std_score_time': array([0.04448159, 0.05933732, 0.01632386, 0.36040775, 0.09999189,\n",
       "        0.01771238, 0.01375356, 0.03272868, 0.14955236, 0.05439001,\n",
       "        0.01068735, 0.10750428, 0.0126197 , 0.04721273, 0.06443183,\n",
       "        0.05096119, 0.02123879, 0.02591057, 0.08836739, 0.06871854,\n",
       "        0.01042494, 0.05873377, 0.03357828, 0.07221089, 0.01703265,\n",
       "        0.03331287, 0.03263182, 0.11395279, 0.05252657, 0.02112699]),\n",
       " 'std_test_score': array([0.00137891, 0.00130521, 0.00145892, 0.00145552, 0.00134109,\n",
       "        0.00126208, 0.00149172, 0.00134202, 0.00128145, 0.00142239,\n",
       "        0.001315  , 0.001427  , 0.00135292, 0.00135513, 0.00135718,\n",
       "        0.00138077, 0.00137292, 0.00138289, 0.00143362, 0.00138615,\n",
       "        0.00140888, 0.00131981, 0.00134594, 0.00128852, 0.00120412,\n",
       "        0.00126969, 0.00129276, 0.00112094, 0.00125683, 0.0011231 ]),\n",
       " 'std_train_score': array([0.00030033, 0.0002285 , 0.00030617, 0.00028162, 0.00032431,\n",
       "        0.00033036, 0.00025235, 0.00029466, 0.00035113, 0.00033437,\n",
       "        0.00041605, 0.00038545, 0.00036239, 0.00037446, 0.00035799,\n",
       "        0.00032678, 0.00022759, 0.00029447, 0.0002608 , 0.00024748,\n",
       "        0.00034812, 0.00035848, 0.00032638, 0.0003629 , 0.00036303,\n",
       "        0.00034165, 0.00036778, 0.00039917, 0.00041871, 0.00038108])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f5e4fc885f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tree.dot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproportion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-Tpng'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tree.dot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-o'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tree.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-Gdpi=600'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                                 \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_executable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "#output a tree in the best random forest classifier\n",
    "feature_names = [\"ignore 0\",\"# sexual organ\",\"# violent words\",\"family words\",\"# negative adj\",\"ignored this\",\"# CAPS\",\"# swears\",\"# !\",\"# complimentary words\",\"# commas\",\"None\",\"# newlines\",\"# disability\",\"# racial slurs\",\"# ethics slurs\",\"# archaic\",\"# class\",\"# gender\",\"# religion\",\"# nationality\"]\n",
    "class_names = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "export_graphviz(rfs.best_estimator_[250],out_file='tree.dot',feature_names=feature_names,class_names=class_names, rounded=True,proportion=False,precision=2,filled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93297822 0.93197556 0.9333208  0.93448017 0.93219064]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98940937 0.98890769 0.98893902 0.99003572 0.98871968]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96550211 0.95071283 0.96462368 0.95340603 0.95493999]\n",
      "[0.99689801 0.99699201 0.99696068 0.99702316 0.99702316]\n",
      "[0.95920414 0.95998747 0.95892085 0.95948487 0.95989095]\n",
      "[0.99091336 0.99094441 0.99094441 0.99122642 0.99081908]\n"
     ]
    }
   ],
   "source": [
    "#trained 6 logstic regression over 5-fold cross validation\n",
    "lr = []\n",
    "lr.append(LogisticRegression())\n",
    "lr.append(LogisticRegression())\n",
    "lr.append(LogisticRegression())\n",
    "lr.append(LogisticRegression())\n",
    "lr.append(LogisticRegression())\n",
    "lr.append(LogisticRegression())\n",
    "lr_cv_results = []\n",
    "i=0\n",
    "while(i<6):\n",
    "    \n",
    "    lr_cv_result = cross_validate(lr[i],train_x,column(train_y,i),cv=5,return_estimator=True,return_train_score=False)\n",
    "    print(lr_cv_result['test_score'])\n",
    "    lr_cv_results.append(lr_cv_result)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_get_proba(clf,x):\n",
    "    a = []\n",
    "    for i in clf:\n",
    "        \n",
    "        for v in i.predict_proba(x.reshape(1,-1)):\n",
    "            a.append(v[1])\n",
    "       \n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.06653619631479718,0.006303504979727946,0.03012209865857804,0.002867777911128313,0.03399787673255796,0.008155976335154988'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append classifiers and ready to output the probability to kaggle\n",
    "lr_clfs = []\n",
    "lr_clfs.append(lr_cv_results[0]['estimator'][0])\n",
    "lr_clfs.append(lr_cv_results[1]['estimator'][0])\n",
    "lr_clfs.append(lr_cv_results[2]['estimator'][0])\n",
    "lr_clfs.append(lr_cv_results[3]['estimator'][0])\n",
    "lr_clfs.append(lr_cv_results[4]['estimator'][0])\n",
    "lr_clfs.append(lr_cv_results[5]['estimator'][0])\n",
    "\n",
    "\",\".join(str(x) for x in lr_get_proba(lr_clfs,test_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output local\n",
    "k=0\n",
    "lr_f = open(\"logistic_regression.csv\",\"w\")\n",
    "for feat in test_x:\n",
    "    prob = lr_get_proba(lr_clfs,feat)\n",
    "    lr_f.write(str(test.values[k][0]) + \",\" +\",\".join(str(x) for x in prob))\n",
    "    lr_f.write(\"\\n\")\n",
    "    k+=1\n",
    "lr_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94113666 0.94164317 0.94070314]\n",
      "[0.99003591 0.99005471 0.99007314]\n",
      "[0.97283375 0.97407407 0.97382967]\n",
      "[0.99699197 0.99701072 0.99701072]\n",
      "[0.96350886 0.96474968 0.96356389]\n",
      "[0.99133312 0.99133296 0.99142696]\n"
     ]
    }
   ],
   "source": [
    "#train 6 svc classifiers over 5 folds\n",
    "#Careful!!! It took over 10 hours to finish!\n",
    "svc = []\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc.append(svm.SVC(probability=True))\n",
    "svc_cv_results = []\n",
    "j=0\n",
    "while(j<6):\n",
    "    svc_cv_result = cross_validate(svc[j],train_x,column(train_y,j),cv=3,return_estimator=True,return_train_score=False)\n",
    "    print(svc_cv_result['test_score'])\n",
    "    svc_cv_results.append(svc_cv_result)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.054940068937689636,0.009839087747827628,0.019901577256955728,0.0031495311437181855,0.02683736324712405,0.008556809949554831'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output the probaility of test set to local\n",
    "svc_clfs = []\n",
    "svc_clfs.append(svc_cv_results[0]['estimator'][0])\n",
    "svc_clfs.append(svc_cv_results[1]['estimator'][0])\n",
    "svc_clfs.append(svc_cv_results[2]['estimator'][0])\n",
    "svc_clfs.append(svc_cv_results[3]['estimator'][0])\n",
    "svc_clfs.append(svc_cv_results[4]['estimator'][0])\n",
    "svc_clfs.append(svc_cv_results[5]['estimator'][0])\n",
    "\n",
    "count = 0\n",
    "for classifier in svc_clfs:\n",
    "    with open(\"svc\"+str(count)+\".pickle\",\"wb\") as f:\n",
    "        pickle.dump(classifier,f)\n",
    "    count+=1\n",
    "\",\".join(str(x) for x in lr_get_proba(svc_clfs,test_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "lr_f = open(\"support_vector_machine.csv\",\"w\")\n",
    "for feat in test_x:\n",
    "    prob = lr_get_proba(svc_clfs,feat)\n",
    "    lr_f.write(str(test.values[k][0]) + \",\" +\",\".join(str(x) for x in prob))\n",
    "    lr_f.write(\"\\n\")\n",
    "    k+=1\n",
    "lr_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when ready, set this to the best model you found, trained on the test data:\n",
    "best_classifier = None\n",
    "\n",
    "with open('classifier.pickle', 'wb') as f: # 'wb' stands for 'write bytes'\n",
    "    pickle.dump(best_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split the sample into 70/30, 70% as training and 30% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x,train_y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4591,  485, 2527,  131, 2362,  430])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each label's sample size in the test set\n",
    "print(len(y_test))\n",
    "np.sum(y_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the accuracy of each label\n",
    "from sklearn.metrics import accuracy_score\n",
    "def average_accuracy(clf):\n",
    "    a = []\n",
    "    c = []\n",
    "    for x in range(len(X_test)):\n",
    "        \n",
    "        pred = clf.predict(X_test[x].reshape(1,-1))\n",
    "        b = [1 if pred[0][y]==y_test[x][y] else 0 for y in range(6)]\n",
    "        c.append(accuracy_score(y_test[x],pred[0]))\n",
    "        \n",
    "        a.append(b)\n",
    "    print(np.mean(c))\n",
    "    return [k/len(y_test) for k in np.sum(a,axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9050593248663101\n",
      "0.9720992090017826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9280790441176471,\n",
       " 0.9898688168449198,\n",
       " 0.9655539772727273,\n",
       " 0.9972635360962567,\n",
       " 0.9608121657754011,\n",
       " 0.9910177139037433]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each label's accuracy for tree classifier over 70/30 validation\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=3)\n",
    "tree.fit(X_train,y_train)\n",
    "\n",
    "print(accuracy_score(y_test,tree.predict(X_test)))\n",
    "average_accuracy(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763222760695187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9420538101604278,\n",
       " 0.9899314839572193,\n",
       " 0.9736380347593583,\n",
       " 0.9972635360962567,\n",
       " 0.963903743315508,\n",
       " 0.9911430481283422]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each label's accuracy for random forest classifier over 70/30 validation\n",
    "rf = RandomForestClassifier(n_estimators=300,min_samples_leaf=4)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "average_accuracy(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 label's accuracy\n",
    "def lr_average_accuracy(clf,i):\n",
    "    a = []\n",
    "    c = []\n",
    "    for x in range(len(X_test)):\n",
    "        \n",
    "        pred = clf.predict(X_test[x].reshape(1,-1))\n",
    "        \n",
    "        b = [1 if pred[0]==y_test[x][i] else 0 ]\n",
    "        \n",
    "        \n",
    "        a.append(b)\n",
    "    \n",
    "    return [k/len(y_test) for k in np.sum(a,axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = []\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,0)))\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,1)))\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,2)))\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,3)))\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,4)))\n",
    "lr.append(LogisticRegression().fit(X_train,column(y_train,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9323404077540107], [0.9886363636363636], [0.965386864973262], [0.9972217580213903], [0.9596632687165776], [0.9907670454545454]]\n"
     ]
    }
   ],
   "source": [
    "#each label's accuracy for logsitic classifier over 70/30 validation\n",
    "lr_performance= [lr_average_accuracy(lr[x],x) for x in range(6)]\n",
    "print(lr_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928747493315508\n",
      "[[0.928747493315508], [0.9898688168449198], [0.9643841911764706], [0.9972635360962567], [0.9582637032085561], [0.9910177139037433]]\n"
     ]
    }
   ],
   "source": [
    "#each label's accuracy for svm classifier over 70/30 validation\n",
    "svc = []\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,0)))\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,1)))\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,2)))\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,3)))\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,4)))\n",
    "svc.append(svm.SVC(probability=False).fit(X_train,column(y_train,5)))\n",
    "\n",
    "print(svc[0].score(X_test,column(y_test,0)))\n",
    "\n",
    "svc_performance= [lr_average_accuracy(svc[x],x) for x in range(6)]\n",
    "print(svc_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09590157085561497, 0.010131183155080214, 0.05278659759358289, 0.0027364639037433156, 0.0493399064171123, 0.008982286096256684]\n"
     ]
    }
   ],
   "source": [
    "#sample size of test set in percentage\n",
    "sample_percentage= [x/len(y_test) for x in np.sum(y_test,axis=0)]\n",
    "print(sample_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111699\n",
      "[10703  1110  5922   347  5515   975]\n",
      "[0.09582001629378956, 0.009937421104933796, 0.05301748448956571, 0.0031065631742450694, 0.04937376341775665, 0.00872881583541482]\n"
     ]
    }
   ],
   "source": [
    "#sample size of train set in percentage\n",
    "print(len(y_train))\n",
    "print(np.sum(y_train,axis=0))\n",
    "train_sample_percentage= [x/len(y_train) for x in np.sum(y_train,axis=0)]\n",
    "print(train_sample_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
