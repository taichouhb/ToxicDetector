{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solve ToxicDetection through feature engineer into logistic regression\n",
    "# Reference this tutorial for the logistic regression: \n",
    "##  https://www.kaggle.com/negation/pytorch-logistic-regression-tutorial\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Input Data needs to be in array of size N x (2)\n",
    "import utils \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "root_dir = os.path.abspath('.')\n",
    "data_dir = os.path.join(root_dir, 'dataset')\n",
    "train = pd.read_csv(os.path.join(data_dir,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "\n",
    "train_x, train_y = utils.featurize(train)\n",
    "sampleIdx = np.random.choice(np.arange(len(train_y)), 15000, replace = False)\n",
    "val_x, val_y= train_x[sampleIdx], train_y[sampleIdx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=2, hidden_dim=1024):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        m = nn.Softmax()\n",
    "        n = nn.functional.leaky_relu(self.linear(x))\n",
    "        n2 = self.linear2(n)\n",
    "        \n",
    "        return m(n2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/500], Loss: 1.7387\n",
      "Epoch: [10/500], Loss: 1.6776\n",
      "Epoch: [15/500], Loss: 1.6492\n",
      "Epoch: [20/500], Loss: 1.6442\n",
      "Epoch: [25/500], Loss: 1.5925\n",
      "Epoch: [30/500], Loss: 1.5432\n",
      "Epoch: [35/500], Loss: 1.5261\n",
      "Epoch: [40/500], Loss: 1.5264\n",
      "Epoch: [45/500], Loss: 1.4983\n",
      "Epoch: [50/500], Loss: 1.4769\n",
      "Epoch: [55/500], Loss: 1.4767\n",
      "Epoch: [60/500], Loss: 1.5307\n",
      "Epoch: [65/500], Loss: 1.4810\n",
      "Epoch: [70/500], Loss: 1.4390\n",
      "Epoch: [75/500], Loss: 1.4498\n",
      "Epoch: [80/500], Loss: 1.3895\n",
      "Epoch: [85/500], Loss: 1.4187\n",
      "Epoch: [90/500], Loss: 1.4641\n",
      "Epoch: [95/500], Loss: 1.4477\n",
      "Epoch: [100/500], Loss: 1.3612\n",
      "Epoch: [105/500], Loss: 1.4020\n",
      "Epoch: [110/500], Loss: 1.4398\n",
      "Epoch: [115/500], Loss: 1.3930\n",
      "Epoch: [120/500], Loss: 1.4204\n",
      "Epoch: [125/500], Loss: 1.4164\n",
      "Epoch: [130/500], Loss: 1.4009\n",
      "Epoch: [135/500], Loss: 1.3658\n",
      "Epoch: [140/500], Loss: 1.3393\n",
      "Epoch: [145/500], Loss: 1.3383\n",
      "Epoch: [150/500], Loss: 1.4014\n",
      "Epoch: [155/500], Loss: 1.3667\n",
      "Epoch: [160/500], Loss: 1.4042\n",
      "Epoch: [165/500], Loss: 1.3153\n",
      "Epoch: [170/500], Loss: 1.3265\n",
      "Epoch: [175/500], Loss: 1.3114\n",
      "Epoch: [180/500], Loss: 1.3382\n",
      "Epoch: [185/500], Loss: 1.3949\n",
      "Epoch: [190/500], Loss: 1.3361\n",
      "Epoch: [195/500], Loss: 1.3597\n",
      "Epoch: [200/500], Loss: 1.3011\n",
      "Epoch: [205/500], Loss: 1.2460\n",
      "Epoch: [210/500], Loss: 1.3574\n",
      "Epoch: [215/500], Loss: 1.3209\n",
      "Epoch: [220/500], Loss: 1.3227\n",
      "Epoch: [225/500], Loss: 1.3105\n",
      "Epoch: [230/500], Loss: 1.3713\n",
      "Epoch: [235/500], Loss: 1.3038\n",
      "Epoch: [240/500], Loss: 1.3197\n",
      "Epoch: [245/500], Loss: 1.2777\n",
      "Epoch: [250/500], Loss: 1.2878\n",
      "Epoch: [255/500], Loss: 1.3318\n",
      "Epoch: [260/500], Loss: 1.3228\n",
      "Epoch: [265/500], Loss: 1.2870\n",
      "Epoch: [270/500], Loss: 1.2089\n",
      "Epoch: [275/500], Loss: 1.2575\n",
      "Epoch: [280/500], Loss: 1.3336\n",
      "Epoch: [285/500], Loss: 1.3367\n",
      "Epoch: [290/500], Loss: 1.3085\n",
      "Epoch: [295/500], Loss: 1.3297\n",
      "Epoch: [300/500], Loss: 1.3244\n",
      "Epoch: [305/500], Loss: 1.2976\n",
      "Epoch: [310/500], Loss: 1.3061\n",
      "Epoch: [315/500], Loss: 1.3249\n",
      "Epoch: [320/500], Loss: 1.2405\n",
      "Epoch: [325/500], Loss: 1.2352\n",
      "Epoch: [330/500], Loss: 1.2838\n",
      "Epoch: [335/500], Loss: 1.2933\n",
      "Epoch: [340/500], Loss: 1.2852\n",
      "Epoch: [345/500], Loss: 1.3123\n",
      "Epoch: [350/500], Loss: 1.3304\n",
      "Epoch: [355/500], Loss: 1.2015\n",
      "Epoch: [360/500], Loss: 1.2825\n",
      "Epoch: [365/500], Loss: 1.2941\n",
      "Epoch: [370/500], Loss: 1.2391\n",
      "Epoch: [375/500], Loss: 1.2764\n",
      "Epoch: [380/500], Loss: 1.2968\n",
      "Epoch: [385/500], Loss: 1.2537\n",
      "Epoch: [390/500], Loss: 1.1972\n",
      "Epoch: [395/500], Loss: 1.2855\n",
      "Epoch: [400/500], Loss: 1.2768\n",
      "Epoch: [405/500], Loss: 1.2815\n",
      "Epoch: [410/500], Loss: 1.2457\n",
      "Epoch: [415/500], Loss: 1.2256\n",
      "Epoch: [420/500], Loss: 1.2662\n",
      "Epoch: [425/500], Loss: 1.2872\n",
      "Epoch: [430/500], Loss: 1.2368\n",
      "Epoch: [435/500], Loss: 1.2643\n",
      "Epoch: [440/500], Loss: 1.2597\n",
      "Epoch: [445/500], Loss: 1.2146\n",
      "Epoch: [450/500], Loss: 1.3142\n",
      "Epoch: [455/500], Loss: 1.2399\n",
      "Epoch: [460/500], Loss: 1.2129\n",
      "Epoch: [465/500], Loss: 1.2581\n",
      "Epoch: [470/500], Loss: 1.2388\n",
      "Epoch: [475/500], Loss: 1.2509\n",
      "Epoch: [480/500], Loss: 1.2595\n",
      "Epoch: [485/500], Loss: 1.2112\n",
      "Epoch: [490/500], Loss: 1.2102\n",
      "Epoch: [495/500], Loss: 1.2776\n",
      "Epoch: [500/500], Loss: 1.2905\n"
     ]
    }
   ],
   "source": [
    "#train on data_train with several epochs\n",
    "epochs = 500 #Can also be changed. The higher this value, the more overfit. \n",
    "feature_dims = 21\n",
    "batchSize = 64\n",
    "classes = 6 #Binaray for toxic, or not\n",
    "model = LogisticRegression(feature_dims, num_classes=classes)\n",
    "#model = model.cuda() #COMMENT OUT TO RUN ON GPU\n",
    "#Todo: set loss criterion and optimizer\n",
    "#Run loop of number of epoch, forward and backpass, loss then into optimizer\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "#Training\n",
    "for epoch in range(epochs):\n",
    "    sampleIdx = np.random.choice(np.arange(len(train_y)), batchSize, replace=False)\n",
    "    x_s = torch.from_numpy(train_x[sampleIdx]).float().to(device)\n",
    "    y_s = torch.from_numpy(train_y[sampleIdx]).to(device)\n",
    "    #for i in range(batchSize): \n",
    "        #print(x_s[i].shape, y_s[i].shape)\n",
    "    optimizer.zero_grad()\n",
    "    scores = model(x_s)\n",
    "    \n",
    "\n",
    "    loss = lossFn(scores, torch.max(y_s,1)[1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 5 == 0:\n",
    "            print ('Epoch: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, epochs,  loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "sampleIdx = np.random.choice(np.arange(len(train_y)), 15000, replace = False)\n",
    "val_x, val_y= train_x[sampleIdx], train_y[sampleIdx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  tensor([0.9278, 0.0146, 0.0143, 0.0155, 0.0132, 0.0145], grad_fn=<SoftmaxBackward>)\n",
      "test:  [1 0 1 0 1 1]\n",
      "predict:  tensor([1.0000e+00, 7.9817e-07, 8.6995e-07, 6.9314e-07, 9.9553e-07, 1.1195e-06],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.9990, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.8913, 0.0219, 0.0212, 0.0212, 0.0220, 0.0223], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.3481, 0.1343, 0.1279, 0.1351, 0.1265, 0.1281], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.7488, 0.0507, 0.0491, 0.0504, 0.0495, 0.0514], grad_fn=<SoftmaxBackward>)\n",
      "test:  [1 0 0 0 0 0]\n",
      "predict:  tensor([0.9493, 0.0103, 0.0097, 0.0104, 0.0100, 0.0102], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.9860, 0.0028, 0.0027, 0.0026, 0.0028, 0.0029], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.5333, 0.0953, 0.0905, 0.0969, 0.0903, 0.0937], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n",
      "predict:  tensor([0.9956, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009], grad_fn=<SoftmaxBackward>)\n",
      "test:  [0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(val_x)):\n",
    "    scores = model(torch.from_numpy(val_x[i]).float().to(device))\n",
    "    if(count>=10):\n",
    "        break\n",
    "    count+=1\n",
    "    print(\"predict: \",scores)\n",
    "    print(\"test: \",val_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#Optimize hyperparameters like batchsize, or learning rate or\n",
    "#Try to add dropout, batch normalization to get highest val acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: \n",
    "#Now try test set, hopefully for accuracy as good as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
